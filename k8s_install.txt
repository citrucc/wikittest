Your company wants to transition to a Kubernetes-based infrastructure for running their containerized applications. In order to do that, they will need a basic Kubernetes cluster to get started.2

You have been provided with three servers. Build a simple Kubernetes cluster with one control plane node and two worker nodes.

Install and use kubeadm to build a Kubernetes cluster on these servers.
Install Kubernetes version 1.24.0.
Use containerd for your container runtime.
The cluster should have one control plane node and two worker nodes.
Use the Calico networking add-on to provide networking for the cluster.
Note: If you would like the lab setup to be the same as the CKA "Building a Kubernetes Cluster" lesson, you can also set the hostnames for each node with command 
sudo hostnamectl set-hostname [k8s-control | k8s-worker1 | k8s-worker2]. 
You would then place the three entries in each node's hosts file with sudo vi /etc/hosts.
The hosts file should contain the private IP of each node along with its corresponding hostname.


Log in to the lab servers using the credentials provided:

ssh cloud_user@<PUBLIC_IP_ADDRESS>
Install Packages
Log in to the control plane node.

Note: The following steps must be performed on all three nodes.

Create configuration file for containerd:

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

Load modules:

sudo modprobe overlay
sudo modprobe br_netfilter
Set system configurations for Kubernetes networking:

cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

Apply new settings:

sudo sysctl --system

Install containerd:

sudo apt-get update && sudo apt-get install -y containerd.io
Create default configuration file for containerd:

sudo mkdir -p /etc/containerd
Generate default containerd configuration and save to the newly created default file:

sudo containerd config default | sudo tee /etc/containerd/config.toml
Restart containerd to ensure new configuration file usage:

sudo systemctl restart containerd
Verify that containerd is running:

sudo systemctl status containerd
Disable swap:

sudo swapoff -a
Install dependency packages:

sudo apt-get update && sudo apt-get install -y apt-transport-https curl
Download and add GPG key:

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Add Kubernetes to repository list:

cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
Update package listings:

sudo apt-get update
Install Kubernetes packages (Note: If you get a dpkg lock message, just wait a minute or two before trying the command again):

sudo apt-get install -y kubelet=1.24.0-00 kubeadm=1.24.0-00 kubectl=1.24.0-00
Turn off automatic updates:

sudo apt-mark hold kubelet kubeadm kubectl
Log in to both worker nodes to perform previous steps.

Initialize the Cluster
On the control plane node, initialize the Kubernetes cluster on the control plane node using kubeadm:

sudo kubeadm init --pod-network-cidr 192.168.0.0/16 --kubernetes-version 1.24.0
Set kubectl access:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
Test access to cluster:

kubectl get nodes
Install the Calico Network Add-On
On the control plane node, install Calico Networking:

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
Check status of the control plane node:

kubectl get nodes
Join the Worker Nodes to the Cluster
In the control plane node, create the token and copy the kubeadm join command:

kubeadm token create --print-join-command
Note: This output will be used as the next command for the worker nodes.

Copy the full output from the previous command used in the control plane node. This command starts with kubeadm join.

In both worker nodes, paste the full kubeadm join command to join the cluster. Use sudo to run it as root:

sudo kubeadm join... 
In the control plane node, view cluster status:

kubectl get nodes


===============================

Most of these commands need to be run on each of the nodes. Pay attention though. Down at Step 10, we are going to do a little bit on just the master, and down at Step 15 we'll run something on just the nodes. There are notes down there, just be watching for them.

1 - Once we have logged in, we need to elevate privileges using sudo:

sudo su  
2 - Disable SELinux:

setenforce 0
sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
3 - Enable the br_netfilter module for cluster communication:

modprobe br_netfilter
echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
4 - Ensure that the Docker dependencies are satisfied:

yum install -y yum-utils device-mapper-persistent-data lvm2
5 - Add the Docker repo and install Docker:

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce
6 - Set the cgroup driver for Docker to systemd, reload systemd, then enable and start Docker:

sed -i '/^ExecStart/ s/$/ --exec-opt native.cgroupdriver=systemd/' /usr/lib/systemd/system/docker.service
systemctl daemon-reload
systemctl enable docker --now
7 - Add the Kubernetes repo:

cat << EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
  https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
8 - Install Kubernetes v1.14.0:

yum install -y kubelet-1.14.0-0 kubeadm-1.14.0-0 kubectl-1.14.0-0 kubernetes-cni-0.7.5
9 - Enable the kubelet service. The kubelet service will fail to start until the cluster is initialized, this is expected:

systemctl enable kubelet
Note: Complete the following section on the MASTER ONLY!
10 - Initialize the cluster using the IP range for Flannel:

kubeadm init --pod-network-cidr=10.244.0.0/16
11 - Copy the kubeadmn join command that is in the output. We will need this later.

12 - Exit sudo, copy the admin.conf to your home directory, and take ownership.

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
13 - Deploy Flannel:

kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel-old.yaml
14 - Check the cluster state:

kubectl get pods --all-namespaces
Note: Complete the following steps on the NODES ONLY!
15 - Run the join command that you copied earlier, this requires running the command prefaced with sudo on the nodes (if we hadn't run sudo su to begin with). Then we'll check the nodes from the master.

kubectl get nodes
Create and Scale a Deployment Using kubectl
Note: These commands will only be run on the master node.
16 - Create a simple deployment:

kubectl create deployment nginx --image=nginx
17 - Inspect the pod:

kubectl get pods
18 - Scale the deployment:

kubectl scale deployment nginx --replicas=4
19 - Inspect the pods. We should have four now:

kubectl get pods

====================================

Solution
Log in to all three servers using the credentials on the lab page (either in your local terminal, using the Instant Terminal feature, or using the public IPs), and work through the objectives listed.

Get the Docker gpg, and add it to your repository.
In all three terminals, run the following command to get the Docker gpg key:

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
Then add it to your repository:

sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
Get the Kubernetes gpg key, and add it to your repository.
In all three terminals, run the following command to get the Kubernetes gpg key:

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
Then add it to your repository:

cat << EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
Update the packages:

sudo apt update
Install Docker, kubelet, kubeadm, and kubectl.
In all three terminals, run the following command to install Docker, kubelet, kubeadm, and kubectl:

sudo apt install -y docker-ce=5:19.03.10~3-0~ubuntu-focal kubelet=1.18.5-00 kubeadm=1.18.5-00 kubectl=1.18.5-00
Initialize the Kubernetes cluster.
In the Controller server terminal, run the following command to initialize the cluster using kubeadm:

sudo kubeadm init --pod-network-cidr=10.244.0.0/16
Set up local kubeconfig.
In the Controller server terminal, run the following commands to set up local kubeconfig:

sudo mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
Apply the flannel CNI plugin as a network overlay.
In the Controller server terminal, run the following command to apply flannel:

kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml
Join the worker nodes to the cluster, and verify they have joined successfully.
When we ran sudo kubeadm init on the Controller node, there was a kubeadmin join command in the output. You'll see it right under this text:

You can now join any number of machines by running the following on each node as root:
To join worker nodes to the cluster, we need to run that command, as root (we'll just preface it with sudo) on each of them. It should look something like this:

sudo kubeadm join <your unique string from the output of kubeadm init>
Run a deployment that includes at least one pod, and verify it was successful.
In the Controller server terminal, run the following command to run a deployment of ngnix:

kubectl create deployment nginx --image=nginx
Verify its success:

kubectl get deployments
Verify the pod is running and available.
In the Controller server terminal, run the following command to verify the pod is up and running:

kubectl get pods
Use port forwarding to extend port 80 to 8081, and verify access to the pod directly.
In the Controller server terminal, run the following command to forward the container port 80 to 8081 (replace <pod_name> with the name in the output from the previous command):

kubectl port-forward <pod_name> 8081:80
Open a new terminal session and log in to the Controller server. Then, run this command to verify we can access this container directly:

curl -I http://127.0.0.1:8081
We should see a status of OK.

Execute a command directly on a pod.
In the original Controller server terminal, hit Ctrl+C to exit out of the running program.

Still in Controller, execute the nginx version command from a pod (using the same <pod_name> as before):

kubectl exec -it <pod_name> -- nginx -v
Create a service, and verify connectivity on the node port.
In the original Controller server terminal, run the following command to create a NodePort service:

kubectl expose deployment nginx --port 80 --type NodePort
View the service:

kubectl get services
Get the node the pod resides on.

kubectl get po -o wide
Verify the connectivity by using curl on the NODE from the previous step and the port from when we viewed the service. Make sure to replace YOUR_NODE and YOUR_PORT with appropriate values for this lab.

curl -I YOUR_NODE:YOUR_PORT
We should see a status of OK.

Conclusion
Congratulations on completing this lab!